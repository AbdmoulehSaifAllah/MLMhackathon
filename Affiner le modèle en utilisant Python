# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uuyD6K3qj7nxXoP52noRNKrI1JRPcuxk
"""

import pandas as pd

# Read in the CSV file
dataset = pd.read_csv("ML-Arxiv-Papers.csv", delimiter="\t")

# Print the first few rows of the dataset
print(dataset.head())
print(dataset)

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping

# Load the data from a CSV file
data = pd.read_csv("ML-Arxiv-Papers.csv")

# Identify missing values in the data
missing_values = data.isnull().sum()

# Print the number of missing values for each column
print(missing_values)

# Replace missing values with a suitable value (e.g. the mean or median)
data.fillna(data.mean(), inplace=True)

# Convert the data to a NumPy array
data_array = data.to_numpy()

# Split the data into training and validation sets
x_train, x_val, y_train, y_val = train_test_split(data_array[:, :-1], data_array[:, -1], test_size=0.2, random_state=42)

# Define the model architecture
model = Sequential([
    Dense(32, activation='relu', input_shape=(x_train.shape[1],)),
    Dense(16, activation='relu'),
    Dense(1, activation='sigmoid')
])

# Define the hyperparameters
learning_rate = 0.001
batch_size = 32
epochs = 100

# Compile the model with an optimizer and loss function
optimizer = Adam(learning_rate=learning_rate)
loss = 'binary_crossentropy'
metrics = ['accuracy']
model.compile(optimizer=optimizer, loss=loss, metrics=metrics)

# Define early stopping
early_stop = EarlyStopping(monitor='val_loss', patience=5)

# Train the model on the training data
history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_val, y_val), callbacks=[early_stop])

# Evaluate the model on the test data
loss, accuracy = model.evaluate(x_val, y_val)
print("Loss: {:.4f}, Accuracy: {:.4f}".format(loss, accuracy))

from flask import Flask, jsonify, request

# Load the saved model weights
model = Sequential()
model.add(Dense(units=1, input_shape=[1]))
model.load_weights('model_weights.h5')

# Set up the Flask app
app = Flask(__name__)

# Define the API endpoint for making predictions
@app.route('/predict', methods=['POST'])
def predict():
    data = request.json
    x = np.array(data['x'])
    y_pred = model.predict(x)
    return jsonify({'y_pred': y_pred.tolist()})


import requests

# Set up the request payload
data = {'x': [1, 2, 3, 4, 5]}

# Send a POST request to the API endpoint
response = requests.post('http://<api-url>/predict', json=data)

# Check the response status code
if response.status_code == 200:
    # Get the predicted values from the response JSON
    y_pred = response.json()['y_pred']
    
    # Print the predicted values
    print('Predicted Values:', y_pred)
else:
    print('Error:', response.text)
